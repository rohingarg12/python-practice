{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 9146200,
          "sourceType": "datasetVersion",
          "datasetId": 5524489
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebookd157e5105b",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohingarg12/python-practice/blob/main/notebookd157e5105b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "sanikatiwarekar_deep_fake_detection_dfd_entire_original_dataset_path = kagglehub.dataset_download('sanikatiwarekar/deep-fake-detection-dfd-entire-original-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "3jMBmkssU4zh"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install facenet-pytorch timm\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "vu0dlsMsU4zi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "FoZ_GYmNU4zj"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from facenet_pytorch import MTCNN\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Init face detector\n",
        "mtcnn = MTCNN(keep_all=False, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Dataset paths\n",
        "real_videos_dir = Path(\"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_original sequences\")\n",
        "fake_videos_dir = Path(\"/kaggle/input/deep-fake-detection-dfd-entire-original-dataset/DFD_manipulated_sequences/DFD_manipulated_sequences\")\n",
        "\n",
        "# Output dirs\n",
        "real_output = Path(\"/kaggle/working/extracted_faces/real\")\n",
        "fake_output = Path(\"/kaggle/working/extracted_faces/fake\")\n",
        "real_output.mkdir(parents=True, exist_ok=True)\n",
        "fake_output.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Extraction function\n",
        "def extract_faces(video_path, output_dir, every_n=15, max_faces=60):\n",
        "    name = video_path.stem\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    count = 0\n",
        "    saved = 0\n",
        "    while cap.isOpened() and saved < max_faces:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "        if count % every_n == 0:\n",
        "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            img = Image.fromarray(rgb)\n",
        "            face = mtcnn(img)\n",
        "            if face is not None:\n",
        "                img = transforms.ToPILImage()(face)\n",
        "                img.save(output_dir / f\"{name}_{saved}.jpg\")\n",
        "                saved += 1\n",
        "        count += 1\n",
        "    cap.release()\n",
        "\n",
        "# Extract\n",
        "print(\"🟢 Extracting Real\")\n",
        "for vid in tqdm(list(real_videos_dir.rglob(\"*.mp4\"))):  # limit for testing\n",
        "    extract_faces(vid, real_output)\n",
        "\n",
        "print(\"🔴 Extracting Fake\")\n",
        "for vid in tqdm(list(fake_videos_dir.rglob(\"*.mp4\"))):  # limit for testing\n",
        "    extract_faces(vid, fake_output)\n",
        "\n",
        "print(\"✅ Done!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-27T08:47:01.074366Z",
          "iopub.execute_input": "2025-03-27T08:47:01.074725Z",
          "iopub.status.idle": "2025-03-27T16:43:01.399156Z",
          "shell.execute_reply.started": "2025-03-27T08:47:01.074694Z",
          "shell.execute_reply": "2025-03-27T16:43:01.3983Z"
        },
        "id": "fWkg1XfUU4zj",
        "outputId": "eb8458c6-0c10-41bd-93e8-6e1c21aa48da"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "🟢 Extracting Real\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 363/363 [57:17<00:00,  9.47s/it] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "🔴 Extracting Fake\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "100%|██████████| 3068/3068 [6:58:30<00:00,  8.18s/it]  ",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Done!\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from pathlib import Path\n",
        "import timm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-27T17:18:03.013426Z",
          "iopub.execute_input": "2025-03-27T17:18:03.013736Z",
          "iopub.status.idle": "2025-03-27T17:18:03.018009Z",
          "shell.execute_reply.started": "2025-03-27T17:18:03.013712Z",
          "shell.execute_reply": "2025-03-27T17:18:03.017196Z"
        },
        "id": "uRNho--OU4zk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths to images\n",
        "real_dir = Path(\"/kaggle/working/extracted_faces/real\")\n",
        "fake_dir = Path(\"/kaggle/working/extracted_faces/fake\")\n",
        "\n",
        "real_paths = list(real_dir.glob(\"*.jpg\"))\n",
        "fake_paths = list(fake_dir.glob(\"*.jpg\"))\n",
        "\n",
        "all_paths = real_paths + fake_paths\n",
        "all_labels = [0] * len(real_paths) + [1] * len(fake_paths)\n",
        "\n",
        "# Train/val split\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(\n",
        "    all_paths, all_labels, test_size=0.2, stratify=all_labels, random_state=42\n",
        ")\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "# Dataset class\n",
        "class FaceDataset(Dataset):\n",
        "    def __init__(self, paths, labels, transform):\n",
        "        self.paths = paths\n",
        "        self.labels = labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self): return len(self.paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
        "        img = self.transform(img)\n",
        "        return img, self.labels[idx]\n",
        "\n",
        "# Loaders\n",
        "train_set = FaceDataset(train_paths, train_labels, transform)\n",
        "val_set = FaceDataset(val_paths, val_labels, transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=8, shuffle=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-27T17:18:13.360196Z",
          "iopub.execute_input": "2025-03-27T17:18:13.360486Z",
          "iopub.status.idle": "2025-03-27T17:18:14.318225Z",
          "shell.execute_reply.started": "2025-03-27T17:18:13.360465Z",
          "shell.execute_reply": "2025-03-27T17:18:14.317537Z"
        },
        "id": "7bUo-bg0U4zl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class MesoNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 8, 3, padding=1), nn.BatchNorm2d(8), nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Conv2d(8, 8, 5, padding=2), nn.BatchNorm2d(8), nn.ReLU(), nn.MaxPool2d(2))\n",
        "        self.fc = nn.Linear(8 * 56 * 56, 256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        return self.fc(x.view(x.size(0), -1))\n",
        "\n",
        "class DeepfakeEnsemble(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
        "        self.resnet.fc = nn.Identity()\n",
        "\n",
        "        self.meso = MesoNet()\n",
        "        self.xception = timm.create_model(\"xception\", pretrained=True, num_classes=0)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(2048 + 256 + 2048, 512),\n",
        "            nn.ReLU(), nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        try:\n",
        "            r = self.resnet(x)\n",
        "            m = self.meso(x)\n",
        "            xcep = self.xception(x)\n",
        "            out = self.classifier(torch.cat([r, m, xcep], dim=1))\n",
        "            return out\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error during forward pass: {e}\")\n",
        "            return None\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-27T17:18:30.233246Z",
          "iopub.execute_input": "2025-03-27T17:18:30.233545Z",
          "iopub.status.idle": "2025-03-27T17:18:30.250566Z",
          "shell.execute_reply.started": "2025-03-27T17:18:30.233515Z",
          "shell.execute_reply": "2025-03-27T17:18:30.249551Z"
        },
        "id": "aIGHddZlU4zl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DeepfakeEnsemble().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "def train(model, train_loader, val_loader, loss_fn, optimizer, device, epochs=3):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
        "            imgs, labels = imgs.to(device), labels.to(device)\n",
        "            outputs = model(imgs)\n",
        "            if outputs is None:\n",
        "                print(\"⛔ Skipping batch due to model error\")\n",
        "                continue\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            total_loss += loss.item()\n",
        "            all_preds += outputs.argmax(1).cpu().tolist()\n",
        "            all_labels += labels.cpu().tolist()\n",
        "\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        print(f\"✅ Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Accuracy: {acc:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_preds, val_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for imgs, labels in val_loader:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                outputs = model(imgs)\n",
        "                if outputs is None:\n",
        "                    continue\n",
        "                val_preds += outputs.argmax(1).cpu().tolist()\n",
        "                val_labels += labels.cpu().tolist()\n",
        "        val_acc = accuracy_score(val_labels, val_preds)\n",
        "        print(f\"🧪 Val Accuracy: {val_acc:.4f}\")\n",
        "        model.train()\n",
        "\n",
        "train(model, train_loader, val_loader, loss_fn, optimizer, device, epochs=3)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-27T18:05:05.025383Z",
          "iopub.execute_input": "2025-03-27T18:05:05.02572Z",
          "iopub.status.idle": "2025-03-27T20:23:36.053243Z",
          "shell.execute_reply.started": "2025-03-27T18:05:05.025691Z",
          "shell.execute_reply": "2025-03-27T20:23:36.052324Z"
        },
        "id": "AUS_yC7HU4zm",
        "outputId": "902a4247-d6de-48d1-c120-aae7fdcc9e68"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name xception to current legacy_xception.\n  model = create_fn(\nEpoch 1: 100%|██████████| 15616/15616 [42:51<00:00,  6.07it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Epoch 1 | Loss: 0.1705 | Accuracy: 0.9374\n🧪 Val Accuracy: 0.9545\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2: 100%|██████████| 15616/15616 [42:50<00:00,  6.07it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Epoch 2 | Loss: 0.1021 | Accuracy: 0.9635\n🧪 Val Accuracy: 0.9638\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3: 100%|██████████| 15616/15616 [42:52<00:00,  6.07it/s]\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Epoch 3 | Loss: 0.0826 | Accuracy: 0.9707\n🧪 Val Accuracy: 0.9674\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"/kaggle/working/deepfake_ensemble.pt\")\n",
        "print(\"✅ Model saved to /kaggle/working/deepfake_ensemble.pt\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-27T20:23:57.349589Z",
          "iopub.execute_input": "2025-03-27T20:23:57.349921Z",
          "iopub.status.idle": "2025-03-27T20:23:57.688826Z",
          "shell.execute_reply.started": "2025-03-27T20:23:57.349897Z",
          "shell.execute_reply": "2025-03-27T20:23:57.687838Z"
        },
        "id": "F5Jt0-oxU4zn",
        "outputId": "2112d3c0-5362-4de0-ef4d-2b4194536719"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ Model saved to /kaggle/working/deepfake_ensemble.pt\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from PIL import Image\n",
        "\n",
        "class VideoFaceSequenceDataset(Dataset):\n",
        "    def __init__(self, root_dir, label_map, max_frames=30, transform=None):\n",
        "        self.root_dir = Path(root_dir)\n",
        "        self.transform = transform\n",
        "        self.max_frames = max_frames\n",
        "        self.samples = []\n",
        "\n",
        "        grouped = defaultdict(list)\n",
        "        for cls, label in label_map.items():\n",
        "            folder = self.root_dir / cls\n",
        "            for img_path in folder.glob(\"*.jpg\"):\n",
        "                video_id = img_path.stem.rsplit(\"_\", 1)[0]\n",
        "                grouped[(video_id, label)].append(img_path)\n",
        "\n",
        "        for (video_id, label), paths in grouped.items():\n",
        "            paths = sorted(paths)[:max_frames]\n",
        "            self.samples.append((video_id, label, paths))\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        _, label, img_paths = self.samples[idx]\n",
        "        frames = []\n",
        "        for path in img_paths:\n",
        "            img = Image.open(path).convert(\"RGB\")\n",
        "            if self.transform:\n",
        "                img = self.transform(img)\n",
        "            frames.append(img)\n",
        "\n",
        "        while len(frames) < self.max_frames:\n",
        "            frames.append(torch.zeros_like(frames[0]))\n",
        "\n",
        "        return torch.stack(frames), label\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-27T20:32:29.819722Z",
          "iopub.execute_input": "2025-03-27T20:32:29.820073Z",
          "iopub.status.idle": "2025-03-27T20:32:29.827357Z",
          "shell.execute_reply.started": "2025-03-27T20:32:29.820046Z",
          "shell.execute_reply": "2025-03-27T20:32:29.826396Z"
        },
        "id": "xobcBl6dU4zo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
        "])\n",
        "\n",
        "label_map = {\"real\": 0, \"fake\": 1}\n",
        "\n",
        "video_dataset = VideoFaceSequenceDataset(\n",
        "    root_dir=\"/kaggle/working/extracted_faces\",\n",
        "    label_map=label_map,\n",
        "    max_frames=30,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "video_loader = DataLoader(video_dataset, batch_size=1, shuffle=True)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-27T20:32:44.010336Z",
          "iopub.execute_input": "2025-03-27T20:32:44.010667Z",
          "iopub.status.idle": "2025-03-27T20:32:45.43853Z",
          "shell.execute_reply.started": "2025-03-27T20:32:44.010637Z",
          "shell.execute_reply": "2025-03-27T20:32:45.437847Z"
        },
        "id": "9pDW8ROOU4zo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoLSTMModel(nn.Module):\n",
        "    def __init__(self, cnn_weights_path, hidden_size=256, num_layers=1, num_classes=2):\n",
        "        super(VideoLSTMModel, self).__init__()\n",
        "        self.cnn = DeepfakeEnsemble(num_classes=2)\n",
        "        self.cnn.load_state_dict(torch.load(cnn_weights_path, map_location='cpu'))\n",
        "        self.cnn.classifier = nn.Identity()\n",
        "        for param in self.cnn.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=2048 + 256 + 2048,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, video_seq):  # [B, T, C, H, W]\n",
        "        embeddings = []\n",
        "        for t in range(video_seq.size(1)):\n",
        "            x = video_seq[:, t].to(device)\n",
        "            with torch.no_grad():\n",
        "                emb = self.cnn(x)\n",
        "            embeddings.append(emb)\n",
        "\n",
        "        seq = torch.stack(embeddings, dim=1)  # [B, T, D]\n",
        "        lstm_out, _ = self.lstm(seq)\n",
        "        final = lstm_out[:, -1, :]\n",
        "        return self.fc(final)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-27T20:33:13.705569Z",
          "iopub.execute_input": "2025-03-27T20:33:13.705912Z",
          "iopub.status.idle": "2025-03-27T20:33:13.712654Z",
          "shell.execute_reply.started": "2025-03-27T20:33:13.705883Z",
          "shell.execute_reply": "2025-03-27T20:33:13.711833Z"
        },
        "id": "ajq9MfZTU4zp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "video_model = VideoLSTMModel(\"/kaggle/working/deepfake_ensemble.pt\").to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(video_model.parameters(), lr=1e-4)\n",
        "\n",
        "def train_video_model(model, dataloader, criterion, optimizer, device, epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        all_preds, all_labels = [], []\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for videos, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
        "            videos, labels = videos.to(device), labels.to(device)\n",
        "            outputs = model(videos)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            all_preds += outputs.argmax(1).cpu().tolist()\n",
        "            all_labels += labels.cpu().tolist()\n",
        "\n",
        "        acc = accuracy_score(all_labels, all_preds)\n",
        "        print(f\"✅ Epoch {epoch+1} | Loss: {total_loss / len(dataloader):.4f} | Accuracy: {acc:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-27T20:33:36.587033Z",
          "iopub.execute_input": "2025-03-27T20:33:36.587338Z",
          "iopub.status.idle": "2025-03-27T20:33:37.618498Z",
          "shell.execute_reply.started": "2025-03-27T20:33:36.587314Z",
          "shell.execute_reply": "2025-03-27T20:33:37.617845Z"
        },
        "id": "r_Vak6d-U4zp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_video_model(video_model, video_loader, criterion, optimizer, device, epochs=3)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-27T20:33:50.906243Z",
          "iopub.execute_input": "2025-03-27T20:33:50.906542Z"
        },
        "id": "YG_TKj14U4zp",
        "outputId": "fd5302a3-2a02-48ac-98ef-768bc3439e21"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Epoch 1:   7%|▋         | 246/3431 [04:01<51:28,  1.03it/s]",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}